{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Machine Learning Assignment 1 - Instructions & Guidelines\n",
    "\n",
    "### **ðŸ“ General Guidelines**\n",
    "Welcome to Machine Learning Assignment 1! This assignment will test your understanding of **regression and classification models**, including **data preprocessing, hyperparameter tuning, and model evaluation**.\n",
    "\n",
    "Follow the instructions carefully, and ensure your implementation is **correct, well-structured, and efficient**.\n",
    "\n",
    "ðŸ”¹ **Submission Format:**  \n",
    "- Your submission **must be a single Jupyter Notebook (.ipynb)** file.  \n",
    "- **File Naming Convention:**  \n",
    "  - Use **your university email as the filename**, e.g.,  \n",
    "    ```\n",
    "    j.doe@innopolis.university.ipynb\n",
    "    ```\n",
    "  - **Do NOT modify this format**, or your submission may not be graded.\n",
    "\n",
    "ðŸ”¹ **Assignment Breakdown:**\n",
    "| Task | Description | Points |\n",
    "|------|------------|--------|\n",
    "| **Task 1.1** | Linear Regression | 20 |\n",
    "| **Task 1.2** | Polynomial Regression | 20 |\n",
    "| **Task 2.1** | Data Preprocessing | 15 |\n",
    "| **Task 2.2** | Model Comparison | 45 |\n",
    "| **Total** | - | **100** |\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“‚ Dataset & Assumptions**\n",
    "The dataset files are stored in the `datasets/` folder.  \n",
    "- **Regression Dataset:** `datasets/task1_data.csv`\n",
    "- **Classification Dataset:** `datasets/pokemon_modified.csv`\n",
    "\n",
    "Each dataset is structured as follows:\n",
    "\n",
    "ðŸ”¹ **`task1_data.csv` (for regression tasks)**  \n",
    "- Contains `X_train`, `y_train`, `X_test`, and `y_test`.  \n",
    "- The goal is to fit **linear and polynomial regression models** and evaluate their performance.  \n",
    "\n",
    "ðŸ”¹ **`pokemon_modified.csv` (for classification tasks)**  \n",
    "- Contains PokÃ©mon attributes, with `is_legendary` as the **binary target variable (0 or 1)**.  \n",
    "- Some features contain **missing values** and **categorical variables**, requiring preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸš€ How to Approach the Assignment**\n",
    "1. **Start with Regression (Task 1)**\n",
    "   - Implement **linear regression** and **polynomial regression**.\n",
    "   - Use **GridSearchCV** for polynomial regression to find the best degree.\n",
    "   - Evaluate using **MSE, RMSE, MAE, and RÂ² Score**.\n",
    "\n",
    "2. **Move to Data Preprocessing (Task 2.1)**\n",
    "   - Load and clean the PokÃ©mon dataset.\n",
    "   - Handle **missing values** correctly.\n",
    "   - Encode categorical variables properly.\n",
    "   - Ensure **no data leakage** when doing the preprocessing.\n",
    "\n",
    "3. **Train and Evaluate Classification Models (Task 2.2)**\n",
    "   - Train **Logistic Regression, KNN, and Naive Bayes**.\n",
    "   - Use **GridSearchCV** for hyperparameter tuning.\n",
    "   - Evaluate models using **Accuracy, Precision, Recall, and F1-score**.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Grading & Evaluation**\n",
    "- Your notebook will be **autograded**, so ensure:\n",
    "  - Your function names **exactly match** the given specifications.\n",
    "  - Your output format matches the expected results.\n",
    "- Partial credit will be given where applicable.\n",
    "\n",
    "ðŸ”¹ **Need Help?**  \n",
    "- If you have any questions, refer to the **assignment markdown instructions** in each task before asking for clarifications.\n",
    "- You can post your question on this [Google sheet](https://docs.google.com/spreadsheets/d/1oyrqXDjT2CeGYx12aZhZ-oDKcQQ-PCgT91wHPhTlBCY/edit?usp=sharing)\n",
    "\n",
    "ðŸš€ **Good luck! Happy coding!** ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ\n",
    "\n",
    "**1) Should we include the lines to import the libraries?**\n",
    "\n",
    "- **Answer:**  \n",
    "  It doesn't matter if you include extra import lines, as the grader will only call the specified functions.\n",
    "\n",
    "**2) Is it okay to submit my file with code outside of the functions?**\n",
    "\n",
    "- **Answer:**  \n",
    "  Yes, you can include additional code outside of the functions as long as the entire script runs correctly when converted to a `.py` file.\n",
    "\n",
    "**Important Clarification:**\n",
    "\n",
    "- The grader will first convert the Jupyter Notebook (.ipynb) into a Python file (.py) and then run it.\n",
    "- **Note:** Please do not include any commands like `!pip install numpy` because they may break the conversion process and therefore the submission will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear and Polynomial Regression (30 Points)\n",
    "\n",
    "### Task 1.1 - Linear Regression (15 Points)\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Train a **linear regression model** on `X_train` and `y_train`.\n",
    "4. Use the trained model to predict `y_test` values.\n",
    "5. Compute and return the following **evaluation metrics** as a dictionary:\n",
    "   - **Mean Squared Error (MSE)**\n",
    "   - **Root Mean Squared Error (RMSE)**\n",
    "   - **Mean Absolute Error (MAE)**\n",
    "   - **RÂ² Score**\n",
    "6. The function signature should match:\n",
    "   ```python\n",
    "   def task1_linear_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do not use any other libraries except for the ones imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import importlib.util\n",
    "import nbformat\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_linear_regression() -> Dict[str, float]:\n",
    "    data = pd.read_csv(\"datasets/task1_data.csv\")\n",
    "    \n",
    "    X_train = data[\"X_train\"].values.reshape(-1, 1)\n",
    "    y_train = data[\"y_train\"].values\n",
    "    X_test = data[\"X_test\"].values.reshape(-1, 1)\n",
    "    y_test = data[\"y_test\"].values\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Polynomial Regression (15 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Define a **pipeline** that includes:\n",
    "   - **Polynomial feature transformation** (degree range: **2 to 10**).\n",
    "   - **Linear regression model**.\n",
    "4. Use **GridSearchCV** with **8-fold cross-validation** to determine the best polynomial degree.\n",
    "5. Train the model with the best polynomial degree and **evaluate it on the test set**.\n",
    "6. Compute and return the following results as a dictionary:\n",
    "   - **Best polynomial degree** (`best_degree`)\n",
    "   - **Mean Squared Error (MSE)**\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task1_polynomial_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_polynomial_regression() -> Dict[str, float]:\n",
    "    data = pd.read_csv(\"datasets/task1_data.csv\")\n",
    "    \n",
    "    X_train = data[\"X_train\"].values.reshape(-1, 1)\n",
    "    y_train = data[\"y_train\"].values\n",
    "    X_test = data[\"X_test\"].values.reshape(-1, 1)\n",
    "    y_test = data[\"y_test\"].values\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures()),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'poly__degree': np.arange(2, 11)\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=8, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"best_degree\": float(grid_search.best_params_['poly__degree']),\n",
    "        \"MSE\": mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classification with Data Preprocessing (70 Points)\n",
    "\n",
    "### Task 2.1 - Data Preprocessing (30 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/pokemon_modified.csv`**.\n",
    "2. Look at the data and study the provided features\n",
    "3. Remove the **two redundant features**\n",
    "4. Handle **missing values**:\n",
    "   - Use **mean imputation** for **\"height_m\"** and **\"weight_kg\"**.\n",
    "   - Use **median imputation** for **\"percentage_male\"**.\n",
    "5. Perform **one-hot encoding** for the categorical column **\"type1\"**.\n",
    "6. Ensure the **target variable** (`\"is_legendary\"`) is present.\n",
    "7. **Split the data into training and testing sets** (`80%-20%` split). Is it balanced?\n",
    "8. **Apply feature scaling** using **StandardScaler** or **MinMaxScaler**.\n",
    "9. Return the following:\n",
    "   - `X_train_scaled`: Processed training features.\n",
    "   - `X_test_scaled`: Processed testing features.\n",
    "   - `y_train`: Training labels.\n",
    "   - `y_test`: Testing labels.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    data = pd.read_csv(\"datasets/pokemon_modified.csv\")\n",
    "    \n",
    "    data = data.drop(columns=['name', 'classification'])\n",
    "    \n",
    "    numerical_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data.select_dtypes(include=[object]).columns\n",
    "    \n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    data[numerical_cols] = imputer_num.fit_transform(data[numerical_cols])\n",
    "    data[categorical_cols] = imputer_cat.fit_transform(data[categorical_cols])\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    encoded_categorical = encoder.fit_transform(data[categorical_cols])\n",
    "    encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    \n",
    "    data = data.drop(columns=categorical_cols)\n",
    "    data = pd.concat([data, encoded_categorical_df], axis=1)\n",
    "    \n",
    "    X = data.drop(columns=['is_legendary'])\n",
    "    y = data['is_legendary']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - Model Comparison (40 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. **Train three classification models** on the preprocessed dataset:\n",
    "   - **Logistic Regression**\n",
    "   - **K-Nearest Neighbors (KNN)**\n",
    "   - **Gaussian Naive Bayes (GNB)**\n",
    "2. Use **GridSearchCV** for **hyperparameter tuning** on:\n",
    "   - **Logistic Regression**: Regularization strength (`C`) and penalty (`l1`, `l2`).\n",
    "   - **KNN**: Number of neighbors (`n_neighbors`), weight function, and distance metric.\n",
    "3. Train each model on the **training set** and evaluate on the **test set**.\n",
    "4. Compute the following **evaluation metrics**:\n",
    "   - **Accuracy**\n",
    "   - **Precision**\n",
    "   - **Recall**\n",
    "   - **F1 Score**\n",
    "5. Return a dictionary containing the evaluation metrics for each model.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:\n",
    "    X_train, X_test, y_train, y_test = task2_preprocessing()\n",
    "    \n",
    "    models = {\n",
    "        \"Logistic Regression\": {\n",
    "            \"model\": LogisticRegression(max_iter=10000),\n",
    "            \"params\": {\n",
    "                \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "                \"penalty\": [\"l1\", \"l2\"],\n",
    "                \"solver\": [\"liblinear\", \"saga\"]\n",
    "            }\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model\": KNeighborsClassifier(),\n",
    "            \"params\": {\n",
    "                \"n_neighbors\": [3, 5, 7, 9, 11],\n",
    "                \"weights\": [\"uniform\", \"distance\"],\n",
    "                \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
    "            }\n",
    "        },\n",
    "        \"Naive Bayes\": {\n",
    "            \"model\": GaussianNB(),\n",
    "            \"params\": {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model_info in models.items():\n",
    "        if model_info[\"params\"]:\n",
    "            grid_search = GridSearchCV(model_info[\"model\"], model_info[\"params\"], cv=5, scoring='accuracy')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "        else:\n",
    "            best_model = model_info[\"model\"]\n",
    "            best_model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=1, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, zero_division=1, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=1, average='weighted')\n",
    "        \n",
    "        results[model_name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
